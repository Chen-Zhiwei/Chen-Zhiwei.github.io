---
title: "PEPNet: Parameter and Embedding Personalized Network for Infusing with Personalized Prior Information"
description: KuaiShou 2023 KDD | 通过多个门单元实现多场景和多任务的推荐系统
date: 2024-05-25
modified_date: 2024-05-26
categories: [Recommender System, Multi-Domain and Multi-Task]
tags: [CTR, Multi-Domain, Multi-Task, KuaiShou, KDD, "2023"]

pin: false
math: true
mermaid: false

render_with_liquid: false

image:
  path: /assets/posts/PEPNet/overview.png
  alt: PEPNet consists of Gate NU, EPNet and PPNet.

---

**通过多个门单元实现多场景和多任务的推荐系统**


### 挑战：解决多场景下多任务的问题

多场景：一个系统中存在多个交互页

多任务：如点击、关注、评论等预测

为什么需要解决多场景多任务的问题？换个言之，为什么需要一个统一的模型解决？

1. 多任务多场景之间是有内部关联的

2. 分离的多个模型(对不同的场景、任务)不符合开发的成本和迭代控制

3. 多个模型没有利用好数据，单个模型的训练数据量可能也会不足

--- 

---

#### 与已有工作的讨论：

已有多场景和多任务的模型，但是并不能简单的进行融合。因为

1 多场景模型集中于对齐多场景之间的语义，忽略了多任务的标签独立性
  
> 理解：考虑一个简单的多场景模型，其输入包括域的id，如果将不同的任务数据一起训练，例如同时进行点击率和评论的预测，这样的模型能够建立多个域之间的联系，但没有考虑标签的独立性
> 

2 多任务模型集中于拟合不同任务的分布，忽略了场景之间的区别

> 理解：考虑一个简单的多任务模型，输入包括任务id，如果将不同任务的数据一起训练，例如同时预测首页广告和猜你喜欢，把所有域的任务都作为训练数据，这样可以实现多任务，但是缺乏了不同域之间的区分
> 

#### **Double Seesaw Phenomenon**

文中提到的不完美跷跷板现象，其实就是多域和多任务之间不能够实现平衡。

---

### **Method**
PEPNet：domain-specific EPNet + task-specific PPNet

#### **Gate NU**

与LHUC的结构类似，通过sigmoid将weight映射为0-2，实现放大特定的语义特征
![pipeline](/assets/posts/PEPNet/GNU.png){: w="900"}

#### **Embedding Personalized Network (EPNet)**

共享Embedding层，输入包括Domain-side feature和General Input，输出包含了域、用户与物品特征的 $O_{ep}$ 向量

1. Domain-side feature，包括domainID，域的统计数据，例如域内的用户数量和曝光的物品
    
2. General Input，包括稀疏特征与密集特征，具体没有说明，应该会包括用户ID、物品ID、用户profile，物品和个人的统计特征等等
    
3. 将域特征和拼接后的物品特征点乘送入Gate NU结构中，得到Personalized（用户、物品、域）的weight
    
4. weight与物品特征进行点乘，得到$$O_{ep}$$送入MMoE集群式的专家模型中
    

#### **Parameter Personalized Network (PPNet)**

共享Embedding层，输入为User-side / Item-side / Author-side的特征，输出为MMoE各层中的weight $O_{pp}$ ,

1. 三种特征的Embeding还要结合$O_{ep}$ ，送入集群式的Gate NU中，其数量与MMoE模型的层数一致
    
2. 每个Gate NU的结果作为weight，与MMoE中**所有**模型的当前层进行点乘，本质上是放大了个性化的特征，而且可能越靠近输出的个性化结果越显著
    

--- 

#### **优化策略**

**缓存中的特征淘汰机制**

采用快手以前也用过的Global Shared Embedding Table (GSET)存储Embedding的结果。因为采用LRU或者LFU，本质上是利用特征的使用频率，但没有考虑特征的固有属性。GSET会为特征打上一个分数，分数较低的其实就是使用频率较低的特征。

**线上同时更新策略**

当模型每次更新时，就更新Embeding特征当然是不可行的。他们用两种策略更新特征，1是为每个特征设置更新的上线次数，2是到期后才更新。

#### **Experiment**

实验对比了多种方法，例如普通的DeepFM，DCN，多任务的MMoE等。

考虑了三个域（双栏发现页、精选视频页、单列流）下的多种任务（点赞、关注、转发、踩、点击、有效查看），实验实验结果基本全部sota，且涨指标效果明显

--- 

**实验细节**

| Setting                          | Value                    |
| :------------------------------- | -----------------------: |
| Batch Size                    | 1024          |
| Embeding Size                   | 40          |
| Initialization                   | Xavier          |
| Networks                   | 2-layer feedforwad NN          |
| Number of experts                   | [4,6,8]          |
| Regularization coefficient | [1e-7, 1e-5, 1e-3]          |

---

对于点击率，超过3秒才为正样本

对于有效查看，超过50%的实践为正样本

训练使用十天的历史数据，第是一天的数据作为评估

---

**如何体现即插即用?**

EPNet是对Embedding做了增强，PPNet是对MMoE模型做了增强

---

**个人总结**: 非常好的工作，因此看的也比较细节，实验很充分，消融上也验证了EPNet和PPNet必要性。

在社区中有复现的模型 [https://github.com/UlionTse/mlgb](https://github.com/UlionTse/mlgb)